---
title: "Advanced Tutorial"
author: "Sean Jewell"
date: '2019-10-02'
output: html_document
layout: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```


Advanced tutorial
----

In this tutorial, we demonstrate advanced use of the ```ChangepointInference``` package. 
Installation instructions are provided [here](software.html). 

First load the package: 
```{r}
require(ChangepointInference)
```

To illustrate the software, we generate a synthetic dataset according to 

$$
\begin{align}
Y_{t} = \mu_{t} + \epsilon_{t}, \quad \epsilon_{t} \overset{\text{iid}}{\sim} \mathrm{N}(0, \sigma^{2}), \quad t=1,\ldots,T,
\end{align}
$$
and assume that $\mu_1,\ldots,\mu_T$ is  piecewise constant, in the sense that  $\mu_{\tau_j+1}=\mu_{\tau_j + 2 } = \ldots = \mu_{\tau_{j+1}}$, $\mu_{\tau_{j+1}} \neq \mu_{\tau_{j+1}+1}$, for $j=0,\ldots,K-1$, where $0 = \tau_{0} < \tau_{1} < \ldots < \tau_{K} < \tau_{K+1} = T$, and where $\tau_1,\ldots,\tau_K$ represent the true changepoints.  


```{r}
set.seed(1)
mu <- rep(c(1, 3, -2), each = 100)
dat <- mu + rnorm(length(mu))
plot(dat, cex = 1, ylab = "y", xlab = "t")
```


#### Changepoint estimation

To estimate changepoints via $\ell_0$ segmentation, we use functional recursions {% cite rigaill2015pruned maidstone2017optimal %}. In this section, we briefly describe these recursions and illustrate how this information can be extracted from our software. 

Let $\mathrm{Cost}(y_{1:s}; u)$ be the cost of segmenting $y_{1:s}$ with $\mu_{s} = u$. Then $\mathrm{Cost}(y_{1:s}; u)$ can be efficiently computed: At the first timepoint, we have $\mathrm{Cost}(y_{1}; u) = \frac12(y_{1} - u)^{2}$; for any $s > 1$ and for all $u$, 
$$
\begin{align}
\mathrm{Cost}(y_{1:s}; u) = \min\left\{ \mathrm{Cost}(y_{1:(s-1)};u), \min_{u'}{\mathrm{Cost}(y_{1:(s-1)};u')} + \lambda	\right\} + \frac12(y_{s} - u)^{2}.
\end{align}
$$


For each $u$, this recursion encapsulates two possibilities: (i) there is no changepoint at  the $(s-1)$st timepoint, and the optimal cost is equal to the previous cost plus the cost of a new data point, $\mathrm{Cost}(y_{1:(s-1)};u) + \frac12(y_{s} - u)^{2}$; (ii) there is a changepoint at the $(s-1)$st timepoint, and the optimal cost is equal to the optimal cost of segmenting up to $s-1$ plus the penalty for adding a changepoint at $s-1$ plus the cost of a new data point,  $\min_{u'}{\mathrm{Cost}(y_{1:(s-1)};u')} + \lambda	+ \frac12(y_{s} - u)^{2}$. 

Setting ```functional_pruning_out = TRUE``` allows us to examine  $\mathrm{Cost}(y_{1:s}; u)$. The following plots $\mathrm{Cost}(y_{1:300}; u)$. Colors represent the most recent changepoint associated with optimal cost at each $u$. 
```{r}
lam <- 4
fit <- changepoint_estimates(dat, "L0", lam, functional_pruning_out = TRUE)
p <- plot(fit, s = 300)
```

To manually access the cost functions use ```fit$piecewise_square_losses```. Since $\mathrm{Cost}(y_{1:s}; u)$ is piecewise quadratic, we represent each component through its coefficients ```(square, linear, constant)``` over domain ```(min_mean, max_mean)```. Furthermore, we store the most recent changepoint ```data_i``` for each region ```(min_mean, max_mean)```. 

For example, the plot above is created by filtering the dataframe to $s = 300$. We see that this cost function is defined over five regions with most recent changepoints at ```200, 287, 294, 299```. 

```{r}
fit$piecewise_square_losses[fit$piecewise_square_losses$s == 300, ]
```


#### Changepoint inference

The conditioning set $\mathcal{S}$ can be extracted for fixed and adaptive $\nu$s by setting ```return_conditioning_sets = TRUE```. See Section 3 of our paper {% cite jewell2019testing %} for additional details. 

For example, 

```{r}
h <- 10
K <- 2
fit_inference <- changepoint_inference(dat, 'BS-fixed', K, window_size = h, sig = 1, return_conditioning_sets = TRUE)
```

The conditioning set for each estimated changepoint can be accessed through

```{r}
fit_inference$conditioning_sets
```
Each row is a subset of $\mathbb{R}$ defined as ```(min_mean, max_mean)```. This region is in $\mathcal{S}$ if ```contained = 1```. 

There are simple plotting tools to visualize these sets: 

```{r}
plot(fit_inference, thj = fit_inference$change_pts[1])
```

In the case of inference with $\ell_0$ segmentation, it is also possible to view the cost of segmenting the data as a function of $\phi$, that is, 

$$
\begin{align}
\mathrm{Cost(\phi)} := \underset{\mu\in\mathbb{R}^{T}}{\mathrm{min}}{\frac12 \sum_{t = 1}^{T} (y_t'(\phi) - \mu_{t})^{2} + \lambda \sum_{t = 
2}^{T} 1_{(\mu_{t} \neq \mu_{t-1})}}.
\end{align}
$$

```{r}
fit_inference <- changepoint_inference(dat, 'L0-fixed', lam, window_size = h, sig = 1, return_conditioning_sets = TRUE)
plot(fit_inference, thj = fit_inference$change_pts[1])
```


References
----

{% bibliography --cited %}